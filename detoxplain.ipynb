{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b563edad-798f-4e9c-a260-0b2c4d3eac48",
   "metadata": {},
   "source": [
    "## Getting Started with the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b62c6-218a-4bef-b191-b6bf98142727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#important libraries for preprocessing using NLTK\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# string manipulation\n",
    "import re\n",
    "\n",
    "# hashtag segmenter\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023b971-dec1-471e-aa11-970df3535387",
   "metadata": {},
   "source": [
    "## Tweets Preprocessing\n",
    "One challenge of this project is to preprocess the twitter dataset for toxicity identification for DistilBERT. For this purpose, the preprocessing is divided and handled into 3 sections: Hashtag extraction, Tweet cleaning, and removal of stop words etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba235b91-b3d7-4838-a961-51148e93531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"FinalBalancedDataset.csv\")\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee18c88",
   "metadata": {},
   "source": [
    "### Tweet cleaning\n",
    "The removal of URLs and @user mentions is done and a new cleaned text is added as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forming a separate feature for cleaned tweets\n",
    "for i,v in enumerate(df[\"tweet\"]):\n",
    "    df.loc[i,\"text\"] = p.clean(df[\"tweet\"][i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1959f1",
   "metadata": {},
   "source": [
    "### Removal of stop words, punctuations etc...\n",
    "Now, we can preprocess the cleaned up tweets in the text column further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "  # removes numbers\n",
    "  data = data.astype(str).str.replace('\\d+', '')\n",
    "  lower_text = data.str.lower()\n",
    "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "  def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(text))]\n",
    "  def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "      new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "      if new_word != '':\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "  words = lower_text.apply(lemmatize_text)\n",
    "  words = words.apply(remove_punctuation)\n",
    "  return pd.DataFrame(words)\n",
    "\n",
    "pre_tweets = preprocess_data(df['text'])\n",
    "df['text'] = pre_tweets\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].apply(lambda x: [item for item in \\\n",
    "                                    x if item not in stop_words])\n",
    "\n",
    "x = df['text'].copy()\n",
    "x_str = []\n",
    "y = df['Toxicity'].copy()\n",
    "y_bool = []\n",
    "for i in range(len(x)):\n",
    "  listToStr1 = ' '.join([str(elem) for elem in \\\n",
    "                                       x[i]])\n",
    "  x_str.append(listToStr1)\n",
    "\n",
    "for i in range(len(y)):\n",
    "  listToStr1 = bool(y[i])\n",
    "  y_bool.append(listToStr1)\n",
    "xlen = [len(t) for t in x_str]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840cde11",
   "metadata": {},
   "source": [
    "## Transformer-based Toxicity Classification Model - DISTILBERT\n",
    "Using DistilBERT, the model is trained on the preprocessed dataset and now we can give an input to it, and get the probability of toxicity back!\n",
    "\n",
    "### Prerequisites to running the model\n",
    "**IMPORTANT:** If you want to skip the training step and use the saved model instead, please run the first 3 cells under here as prerequisites and skip to loading the saved model. In order to do this, you must have the saved model downloaded to appropriate directory, or have already run the training and the model is saved on the appropriate place.\n",
    "\n",
    "Download the saved model from [here](https://drive.google.com/drive/folders/1HB82YQb-W0qGW-ltXTuaoD8tCaj8xyo_?usp=drive_link). Make sure it is under the following directory in your Google Drive: ./gdrive/My Drive/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchtext\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mps if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd370e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "  return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)\n",
    "\n",
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "\n",
    "def create_predictor(model, model_name, max_len):\n",
    "  tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "  def predict_proba(text):\n",
    "      x = [text]\n",
    "\n",
    "      encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "      tfdataset = construct_tfdataset(encodings)\n",
    "      tfdataset = tfdataset.batch(1)\n",
    "\n",
    "      preds = model.predict(tfdataset).logits\n",
    "      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "      return preds[0][0]\n",
    "\n",
    "  return predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8dd3a",
   "metadata": {},
   "source": [
    "### Training and saving the model (optional - no need if you have the model as specified above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23229669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set distilbert model name\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 20\n",
    "\n",
    "# try and see how distilbert tokenization works on input text\n",
    "tweet0 = x[0]\n",
    "tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "inputs = tkzr(tweet0, max_length=MAX_LEN, truncation=True, padding=True)\n",
    "\n",
    "print(f'first tweet: \\'{tweet0}\\'')\n",
    "print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "print(f'attention mask: {inputs[\"attention_mask\"]}')\n",
    "\n",
    "# -------\n",
    "\n",
    "# prepare dataset into training and test\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "encodings = construct_encodings(x_str, tkzr, max_len=MAX_LEN)\n",
    "tfdataset = construct_tfdataset(encodings, y_bool)\n",
    "\n",
    "train_size = int(len(x_str) * (1-TEST_SPLIT))\n",
    "\n",
    "tfdataset = tfdataset.shuffle(len(x_str))\n",
    "tfdataset_train = tfdataset.take(train_size)\n",
    "tfdataset_test = tfdataset.skip(train_size)\n",
    "\n",
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)\n",
    "\n",
    "# -------\n",
    "\n",
    "# distilbert fine-tuning (training)\n",
    "N_EPOCHS = 2\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)\n",
    "\n",
    "# ---------\n",
    "\n",
    "# get accuracy score as evaluation result\n",
    "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
    "print(benchmarks)\n",
    "\n",
    "# ---------\n",
    "\n",
    "clf = create_predictor(model, MODEL_NAME, MAX_LEN)\n",
    "\n",
    "# ---------\n",
    "# test the toxicity identification model by giving text input\n",
    "print(clf('I do not care who wins the election, I do not like politics'))\n",
    "\n",
    "# ---------\n",
    "# save the model to drive\n",
    "model.save_pretrained('model/toxicityidentifier')\n",
    "with open('model/info.pkl', 'wb') as f:\n",
    "    pickle.dump((MODEL_NAME, MAX_LEN), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4f1e4",
   "metadata": {},
   "source": [
    "### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the saved model\n",
    "new_model = TFDistilBertForSequenceClassification.from_pretrained('model/toxicityidentifier')\n",
    "model_name, max_len = pickle.load(open('model/info.pkl', 'rb'))\n",
    "\n",
    "clf = create_predictor(new_model, model_name, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117c47e",
   "metadata": {},
   "source": [
    "## Explainabiliy with LIME\n",
    "LIME gives a blackbox model explanation. It works by removing words in the text to check how effective these words are in model's toxicity classification ability. With LIME, we can highlight toxic words in a given text to explain to the user which words strongly suggested toxic speech in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "#1 - toxic, 0 - nontoxic\n",
    "class_names = [\"non-toxic\", \"toxic\"]\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# alternative function for lime to accept\n",
    "def predict_proba_lime(x):\n",
    "  tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "  encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "  tfdataset = construct_tfdataset(encodings)\n",
    "  tfdataset = tfdataset.batch(1)\n",
    "\n",
    "  preds = new_model.predict(tfdataset).logits\n",
    "  preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "  return preds\n",
    "\n",
    "\n",
    "str_to_predict = \"i hate people when they do that, shut up stupid people, fuck you\"\n",
    "exp = explainer.explain_instance(str_to_predict, predict_proba_lime, num_features=10, num_samples=1000)\n",
    "exp.show_in_notebook(text=str_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8854eb4c",
   "metadata": {},
   "source": [
    "## Toxicity Category Assignment with roBERTa\n",
    "Next, we can give more explainability to toxicity classification with roBERTa that is pre-trained on the Jigsaw Dataset. This model categorizes the toxic text into why it is toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from detoxify import Detoxify\n",
    "\n",
    "def return_key_largest(input_text, toxicity_rate):\n",
    "\n",
    "    results = Detoxify('unbiased').predict(input_text)\n",
    "\n",
    "    if(toxicity_rate < 0.50):\n",
    "        return \"all_good\"\n",
    "    else:\n",
    "        sent_list = list(results.items())[2:]\n",
    "        sent_dict = dict(sent_list)\n",
    "        key_largest = max(sent_dict, key=sent_dict.get)\n",
    "        return key_largest\n",
    "\n",
    "# categories are: obscene, identity_attack, insult, threat, sexual_explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f534c1",
   "metadata": {},
   "source": [
    "## Detoxification Prompting - Different Prompts for Different Categories\n",
    "The prompt is written in accordance to the category of why the text is toxic which is given by roBERTa model.\n",
    "\n",
    "**IMPORTANT**: Due to security reasons, we are unable to provide an API key for you. Please acquire it, and enter it in the place of \"ENTER-YOUR-API-KEY-HERE\", and assign your API key to openai.api_key variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# set your API key\n",
    "#api_key = 'ENTER-YOUR-API-KEY-HERE'\n",
    "\n",
    "# input: text by user\n",
    "# output: prompt by GPT-4o-mini\n",
    "\n",
    "# Using OpenAI API, for summarization\n",
    "def response(prompt, model=\"gpt-4o-mini\"):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        temperature=0.4, # this is the degree of randomness of the model's output\n",
    "        max_output_tokens=512\n",
    "    )\n",
    "    return(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f4502",
   "metadata": {},
   "source": [
    "## Detoxification Prompting Additional Experiment - Parallel Examples\n",
    "The prompt is written to include 10 examples on how to detoxify a text (parallel examples) and GPT is asked to give the 11th detox text which is the detoxified version of the user input. This part is not included into the final system (UI), and the previous prompting technique is included as it works better and informed. This piece of code is there for the sake of completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7240f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def init():\n",
    "  dataset = load_dataset(\"s-nlp/paradetox\")\n",
    "  rows=[]\n",
    "  for i in range(5000):\n",
    "    text = \"Instead of saying \" + '\"' + str(dataset['train'][i]['en_toxic_comment']) + '\"' + \", i can express this in other way like \" + '\"' + str(dataset['train'][i]['en_neutral_comment']) +'\"'\n",
    "    rows.append(text)\n",
    "  df = pd.DataFrame(rows, columns=['texts'])\n",
    "  df = df.sample(frac = 1)\n",
    "  return df\n",
    "\n",
    "def add_few_shot(df, given_text, size=10):\n",
    "  text = \"Complete the last one according to examples given: \" + \"\\n\\n\"\n",
    "  for i in range (size):\n",
    "    text = df['texts'][i] + \"\\n\"\n",
    "  promptWithEx = text + \"Instead of saying \" + '\"' + given_text + '\"' + \", i can express this in other way like \"\n",
    "\n",
    "  completion = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=promptWithEx\n",
    "  )\n",
    "  response = completion.choices[0].text\n",
    "  return response\n",
    "\n",
    "def runDetoxFewShot(given_text):\n",
    "  df = init()\n",
    "  detoxSentence = add_few_shot(df, given_text)\n",
    "  return detoxSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698a936",
   "metadata": {},
   "source": [
    "## GRADIO for User Interface\n",
    "A simple UI for user input and outputs which are toxicity rate by distilBERT, highlighted text by LIME for explainability, toxicity category assignment by roBERTa, GPT prompting in detoxification of the user input. After running, you will see the UI which you can interact with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# alternative function for gradio to accept\n",
    "def predict_proba_gradio(x):\n",
    "\n",
    "  x_org = x\n",
    "  tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "  encodings = construct_encodings([x], tkzr, max_len=max_len)\n",
    "  tfdataset = construct_tfdataset(encodings)\n",
    "  tfdataset = tfdataset.batch(1)\n",
    "\n",
    "  preds = new_model.predict(tfdataset).logits\n",
    "  preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "  exp = explainer.explain_instance(x, predict_proba_lime, num_features=10, num_samples=1000)\n",
    "  highlighted_list = exp.as_list()\n",
    "\n",
    "  # detoxify for categorization\n",
    "  reason = return_key_largest(x_org, preds[0][1])\n",
    "\n",
    "  if(reason == \"obscene\"):\n",
    "    explanation_reason = \"This text contains obscenity, therefore it is deemed as toxic.\"\n",
    "    prompt = f\"The following text contains obscenity, therefore it is deemed as toxic: '{x_org}'. Please rephrase this sentence while keeping the content and style preserved.\"\n",
    "    detox_1 = response(prompt)\n",
    "\n",
    "  elif(reason == \"identity_attack\"):\n",
    "    explanation_reason = \"This text contains an attack on a specific identity, therefore it is deemed as toxic.\"\n",
    "    prompt = f\"The following text contains an attack on a specific identity, therefore it is deemed as toxic: '{x_org}'. Please rephrase this sentence while keeping the content and style preserved.\"\n",
    "    detox_1 = response(prompt)\n",
    "\n",
    "  elif(reason == \"insult\"):\n",
    "    explanation_reason = \"This text contains insult, there it is deemed as toxic.\"\n",
    "    prompt = f\"The following text contains obscentity: '{x_org}'. Please rephrase this sentence while keeping the content and style preserved.\"\n",
    "    detox_1 = response(prompt)\n",
    "\n",
    "  elif(reason == \"threat\"):\n",
    "    explanation_reason = \"This text contains at least one threat, therefore it is deemed as toxic.\"\n",
    "    prompt = f\"This following text contains contains at least one threat, therefore it is deemed as toxic: '{x_org}'. Please rephrase this sentence while keeping the content and style preserved.\"\n",
    "    detox_1 = response(prompt)\n",
    "\n",
    "  elif(reason == \"sexual_explicit\"):\n",
    "    explanation_reason = \"This text contains an sexually explicit content, therefore it is deemed as toxic.\"\n",
    "    prompt = f\"This following text contains contains an sexually explicit content, therefore it is deemed as toxic: '{x_org}'. Please rephrase this sentence while keeping the content and style preserved.\"\n",
    "    detox_1 = response(prompt)\n",
    "\n",
    "  elif(reason == \"all_good\"):\n",
    "    explanation_reason = \"No serious toxicity was found in this text, all good!\"\n",
    "    detox_1 = x_org\n",
    "\n",
    "  return preds[0][1], highlighted_list, explanation_reason, detox_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c082dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradio elements to display such as user text input, and the mentioned outputs of the models\n",
    "# it is advised to follow the running localhost to open in full web page mode\n",
    "input = gr.Textbox(label='Enter Your Text Here')\n",
    "toxic_output = gr.Textbox(label='Toxicity Rate')\n",
    "highlighted_text = gr.HighlightedText(label=\"Where might be the toxicity?: purple - nontoxic | red - toxic\",show_legend=True,)\n",
    "reason_toxic = gr.Textbox(label='What might be the reason?')\n",
    "# shows the text itself if the text is not toxic\n",
    "detox_suggestion_1 = gr.Textbox(label='Does this sound better?')\n",
    "gr.Interface(fn = predict_proba_gradio, inputs = input, outputs = [toxic_output, highlighted_text, reason_toxic, detox_suggestion_1], title = \"DETOXPLAIN\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
